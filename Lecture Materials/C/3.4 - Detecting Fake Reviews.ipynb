{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPXSfkFeBeFI/wFvz207WLd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["#**Detecting Fake Reviews**"],"metadata":{"id":"iqBsw2Dyo1BA"}},{"cell_type":"markdown","source":["This dataset is the product of research by a few folks in Computer Science: https://aclanthology.org/N13-1053.pdf. "],"metadata":{"id":"KIEFBE0apGc3"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"o7R5wj0jhcY1","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1680198450993,"user_tz":240,"elapsed":3448,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"b01f8281-2315-4340-93db-73536766d1bc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       deceptive   hotel  polarity source  \\\n","count       1600    1600      1600   1600   \n","unique         2      20         2      3   \n","top     truthful  conrad  positive  MTurk   \n","freq         800      80       800    800   \n","\n","                                                     text  \n","count                                                1600  \n","unique                                               1596  \n","top     The Omni was chosen for it's location whichwor...  \n","freq                                                    2  "],"text/html":["\n","  <div id=\"df-75b34c2c-535b-45bc-876a-fbbd0f4c358d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>deceptive</th>\n","      <th>hotel</th>\n","      <th>polarity</th>\n","      <th>source</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1600</td>\n","      <td>1600</td>\n","      <td>1600</td>\n","      <td>1600</td>\n","      <td>1600</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>2</td>\n","      <td>20</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1596</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>truthful</td>\n","      <td>conrad</td>\n","      <td>positive</td>\n","      <td>MTurk</td>\n","      <td>The Omni was chosen for it's location whichwor...</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>800</td>\n","      <td>80</td>\n","      <td>800</td>\n","      <td>800</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75b34c2c-535b-45bc-876a-fbbd0f4c358d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-75b34c2c-535b-45bc-876a-fbbd0f4c358d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-75b34c2c-535b-45bc-876a-fbbd0f4c358d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":1}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from google.colab import files\n","import pandas as pd\n","import io\n","\n","trip_advisor = pd.read_csv('https://raw.githubusercontent.com/gburtch/BA865-2023/main/Lecture%20Materials/C/dataset/deceptive-opinion.csv')\n","\n","#uploaded = files.upload()\n","#trip_advisor = pd.read_csv(io.BytesIO(uploaded['deceptive-opinion.csv']))\n","trip_advisor.describe(include='all')"]},{"cell_type":"markdown","source":["We can use some of the Keras utilities to pre-process the text. \n","\n","*Q: What features should we use for our prediction?*"],"metadata":{"id":"uFvjAtrpoxb-"}},{"cell_type":"code","source":["import numpy as np\n","from keras.preprocessing.text import text_to_word_sequence\n","\n","# The dataset is perfectly balanced, so 50% accuracy will be equivalent to a random guess. \n","labels = np.where(trip_advisor['deceptive']=='truthful',0,1)\n","\n","text = []\n","for i in range(len(trip_advisor)):\n","  text.append(text_to_word_sequence(trip_advisor['text'][i])) # This strips punctuation, odd characters, and makes things lower-case. "],"metadata":{"id":"_8dsXMrbk445","executionInfo":{"status":"ok","timestamp":1680198487339,"user_tz":240,"elapsed":160,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["This is another pre-processing step, but it's optional. \n","\n","*Q: What does this code block do?*"],"metadata":{"id":"PFNRsZqu3JNl"}},{"cell_type":"code","source":["min_freq = 1\n","\n","word_freq = {}\n","for review in text:\n","  for term in review:\n","    try:\n","        word_freq[term] = word_freq[term]+1\n","    except KeyError:\n","        word_freq[term] = 1\n","\n","max_freq = max(i for i in word_freq.values())\n","for i in range(len(text)):\n","  text[i] = [term for term in text[i] if word_freq[term] >= min_freq & word_freq[term] <= max_freq]"],"metadata":{"id":"kySkp5Gi2_yv","executionInfo":{"status":"ok","timestamp":1680198494000,"user_tz":240,"elapsed":408,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Here, we are making our integer codings for the text tokens."],"metadata":{"id":"E4bJC8IWkkKa"}},{"cell_type":"code","source":["# We declare a set, which we populate from terms from the corpus, one by one. \n","# Sets only allow 'unique' values. \n","unique_terms = {term for review in text for term in review}\n","print(f'We have {len(unique_terms)} unique tokens in our dataset.')\n","\n","# We can then easily make a term-integer dictionary and an integer-term dictionary (for reverse lookup)\n","word_index = {term: number for number, term in enumerate(unique_terms)}\n","reverse_index = {number: term for number, term in enumerate(unique_terms)}"],"metadata":{"id":"L4XcXwCDki8A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680198497589,"user_tz":240,"elapsed":3,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"9ba550bc-1c7a-4acc-9c6a-fed033f55e64"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["We have 10275 unique tokens in our dataset.\n"]}]},{"cell_type":"markdown","source":["One-hot encoding the text can be done very explicitly now, as a nested loop."],"metadata":{"id":"mvIDdBCaiY1f"}},{"cell_type":"code","source":["def vectorize_sequences(sequences, dimension=len(unique_terms)): \n","    # Make our blank matrix of 0's to store hot encodings.\n","    results = np.zeros((len(sequences), dimension))\n","\n","    # For each observation and element in that observation,\n","    # Update the blank matrix to a 1 at row obs, column element value.\n","    for i, sequence in enumerate(sequences):\n","        for j in sequence:\n","            results[i, word_index[j]] = 1.\n","    return results\n","\n","text_onehot = vectorize_sequences(text)\n","\n","text_onehot.shape"],"metadata":{"id":"NdGsdLNxqyyf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680198502957,"user_tz":240,"elapsed":868,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"89995ded-996e-4f48-c271-712602cecd9b"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1600, 10275)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Don't forget we have a few other features in the data. It's not just about the review text... "],"metadata":{"id":"4MONCGalmvsS"}},{"cell_type":"code","source":["# One hot encoding the hotels. \n","hotel_dict = {hotel : index for index, hotel in enumerate(set(trip_advisor['hotel']))}\n","hotels = []\n","for hotel in trip_advisor['hotel']:\n","  hotels.append(hotel_dict[hotel])\n","\n","hotels_onehot = keras.utils.to_categorical(np.array(hotels))\n","\n","# One hot encoding the review source\n","source_int = np.where(np.array(trip_advisor['source'])=='MTurk',0,np.where(np.array(trip_advisor['source'])=='TripAdvisor',1,2))\n","source_onehot = keras.utils.to_categorical(source_int)\n","\n","# Binarizing the polarity\n","polarity_bin = np.where(np.array(trip_advisor['polarity'])==\"negative\",0,1).reshape(1600,1)\n","\n","# Last step, we shuffle the data\n","data_onehot = np.concatenate((labels.reshape(1600,1),text_onehot,hotels_onehot,polarity_bin),axis=1)\n","np.random.shuffle(data_onehot)\n","\n","# Then we pull out predictors and labels.\n","predictors = data_onehot[:,1:]\n","labels = data_onehot[:,0]\n"],"metadata":{"id":"Jr3WnmXhmzcc","executionInfo":{"status":"ok","timestamp":1680198506965,"user_tz":240,"elapsed":820,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Now we can fit out model to the resulting data. Once again we have k-fold cross validation here. "],"metadata":{"id":"pR9UI8U4zkjm"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","\n","def build_model():\n","    model = keras.Sequential([\n","        # This is essentially a dense layer that mimics word embedding; we are reducing the one-hot encoded text (10,000+ one-hot tokens) down to 750 latent dimensions.\n","        layers.Dense(750, activation=\"linear\"),\n","        layers.Dense(50, activation=\"relu\",kernel_regularizer=\"l2\"),\n","        layers.Dense(5, activation=\"relu\"),\n","        layers.Dense(1, activation=\"sigmoid\")\n","    ])\n","    model.compile(optimizer=keras.optimizers.Adadelta(learning_rate=0.01), loss=\"binary_crossentropy\", metrics=[keras.metrics.BinaryAccuracy(threshold=0.5)])\n","    return model\n","\n","model = build_model()\n","\n","data_train = predictors[:1200]\n","labels_train = labels[:1200]\n","data_test = predictors[1200:]\n","labels_test = labels[1200:]\n","\n","k = 4\n","num_validation_samples = len(data_train) // k\n","num_epochs = 50\n","batch_sizes = 25\n","all_loss_histories = []\n","all_val_loss_histories = []  \n","all_acc_histories = []\n","all_val_acc_histories = []\n","\n","# For each validation fold, we will train a full set of epochs, and store the history. \n","for fold in range(k):\n","    validation_data = data_train[num_validation_samples * fold:\n","                           num_validation_samples * (fold + 1)]\n","    validation_targets = labels_train[num_validation_samples * fold:\n","                           num_validation_samples * (fold + 1)]\n","    training_data = np.concatenate([\n","        data_train[:num_validation_samples * fold],\n","        data_train[num_validation_samples * (fold + 1):]])\n","    training_targets = np.concatenate([\n","        labels_train[:num_validation_samples * fold],\n","        labels_train[num_validation_samples * (fold + 1):]])\n","    model = build_model()\n","    history = model.fit(training_data, training_targets, \n","                        validation_data = (validation_data,validation_targets), \n","                        epochs=num_epochs, batch_size=batch_sizes)\n","    val_loss_history = history.history['val_loss']\n","    val_acc_history = history.history['val_binary_accuracy']\n","    loss_history = history.history['loss']\n","    acc_history = history.history['binary_accuracy']\n","    all_val_loss_histories.append(val_loss_history)\n","    all_loss_histories.append(loss_history)\n","    all_val_acc_histories.append(val_acc_history)\n","    all_acc_histories.append(acc_history)"],"metadata":{"id":"8sEQSBHNiGSo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680198563135,"user_tz":240,"elapsed":45103,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"70b89639-7984-46d0-a94f-cec4984360fc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","36/36 [==============================] - 7s 10ms/step - loss: 1.6317 - binary_accuracy: 0.4978 - val_loss: 1.6183 - val_binary_accuracy: 0.5600\n","Epoch 2/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.6152 - binary_accuracy: 0.5111 - val_loss: 1.6073 - val_binary_accuracy: 0.5733\n","Epoch 3/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5992 - binary_accuracy: 0.5211 - val_loss: 1.5955 - val_binary_accuracy: 0.6100\n","Epoch 4/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5825 - binary_accuracy: 0.5356 - val_loss: 1.5820 - val_binary_accuracy: 0.6200\n","Epoch 5/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5648 - binary_accuracy: 0.5544 - val_loss: 1.5664 - val_binary_accuracy: 0.6067\n","Epoch 6/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5470 - binary_accuracy: 0.5633 - val_loss: 1.5524 - val_binary_accuracy: 0.6200\n","Epoch 7/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5286 - binary_accuracy: 0.5889 - val_loss: 1.5369 - val_binary_accuracy: 0.6267\n","Epoch 8/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5096 - binary_accuracy: 0.6122 - val_loss: 1.5209 - val_binary_accuracy: 0.6367\n","Epoch 9/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4893 - binary_accuracy: 0.6600 - val_loss: 1.5025 - val_binary_accuracy: 0.6667\n","Epoch 10/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4658 - binary_accuracy: 0.7578 - val_loss: 1.4795 - val_binary_accuracy: 0.7400\n","Epoch 11/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4380 - binary_accuracy: 0.8100 - val_loss: 1.4622 - val_binary_accuracy: 0.8067\n","Epoch 12/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4094 - binary_accuracy: 0.8844 - val_loss: 1.4368 - val_binary_accuracy: 0.8367\n","Epoch 13/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3812 - binary_accuracy: 0.9000 - val_loss: 1.4153 - val_binary_accuracy: 0.8400\n","Epoch 14/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3544 - binary_accuracy: 0.9122 - val_loss: 1.3969 - val_binary_accuracy: 0.8533\n","Epoch 15/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3284 - binary_accuracy: 0.9244 - val_loss: 1.3735 - val_binary_accuracy: 0.8567\n","Epoch 16/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3039 - binary_accuracy: 0.9267 - val_loss: 1.3539 - val_binary_accuracy: 0.8600\n","Epoch 17/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2801 - binary_accuracy: 0.9344 - val_loss: 1.3374 - val_binary_accuracy: 0.8667\n","Epoch 18/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2566 - binary_accuracy: 0.9333 - val_loss: 1.3170 - val_binary_accuracy: 0.8667\n","Epoch 19/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2341 - binary_accuracy: 0.9389 - val_loss: 1.3020 - val_binary_accuracy: 0.8767\n","Epoch 20/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2122 - binary_accuracy: 0.9411 - val_loss: 1.2850 - val_binary_accuracy: 0.8700\n","Epoch 21/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1911 - binary_accuracy: 0.9456 - val_loss: 1.2671 - val_binary_accuracy: 0.8767\n","Epoch 22/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1698 - binary_accuracy: 0.9478 - val_loss: 1.2519 - val_binary_accuracy: 0.8700\n","Epoch 23/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1490 - binary_accuracy: 0.9478 - val_loss: 1.2307 - val_binary_accuracy: 0.8833\n","Epoch 24/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1295 - binary_accuracy: 0.9489 - val_loss: 1.2155 - val_binary_accuracy: 0.8767\n","Epoch 25/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1101 - binary_accuracy: 0.9467 - val_loss: 1.2012 - val_binary_accuracy: 0.8733\n","Epoch 26/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0915 - binary_accuracy: 0.9500 - val_loss: 1.1865 - val_binary_accuracy: 0.8767\n","Epoch 27/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0732 - binary_accuracy: 0.9467 - val_loss: 1.1740 - val_binary_accuracy: 0.8767\n","Epoch 28/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0559 - binary_accuracy: 0.9522 - val_loss: 1.1582 - val_binary_accuracy: 0.8800\n","Epoch 29/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0391 - binary_accuracy: 0.9500 - val_loss: 1.1442 - val_binary_accuracy: 0.8767\n","Epoch 30/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0229 - binary_accuracy: 0.9511 - val_loss: 1.1301 - val_binary_accuracy: 0.8800\n","Epoch 31/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0074 - binary_accuracy: 0.9522 - val_loss: 1.1198 - val_binary_accuracy: 0.8900\n","Epoch 32/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9920 - binary_accuracy: 0.9544 - val_loss: 1.1077 - val_binary_accuracy: 0.8900\n","Epoch 33/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9773 - binary_accuracy: 0.9578 - val_loss: 1.0942 - val_binary_accuracy: 0.8833\n","Epoch 34/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9629 - binary_accuracy: 0.9567 - val_loss: 1.0871 - val_binary_accuracy: 0.8867\n","Epoch 35/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9491 - binary_accuracy: 0.9589 - val_loss: 1.0749 - val_binary_accuracy: 0.8933\n","Epoch 36/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9356 - binary_accuracy: 0.9611 - val_loss: 1.0629 - val_binary_accuracy: 0.8933\n","Epoch 37/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9227 - binary_accuracy: 0.9633 - val_loss: 1.0518 - val_binary_accuracy: 0.8933\n","Epoch 38/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9098 - binary_accuracy: 0.9622 - val_loss: 1.0450 - val_binary_accuracy: 0.8867\n","Epoch 39/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8974 - binary_accuracy: 0.9656 - val_loss: 1.0321 - val_binary_accuracy: 0.8933\n","Epoch 40/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8853 - binary_accuracy: 0.9656 - val_loss: 1.0222 - val_binary_accuracy: 0.8900\n","Epoch 41/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8737 - binary_accuracy: 0.9667 - val_loss: 1.0134 - val_binary_accuracy: 0.8900\n","Epoch 42/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8622 - binary_accuracy: 0.9678 - val_loss: 1.0048 - val_binary_accuracy: 0.8900\n","Epoch 43/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8510 - binary_accuracy: 0.9700 - val_loss: 0.9973 - val_binary_accuracy: 0.8867\n","Epoch 44/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8401 - binary_accuracy: 0.9711 - val_loss: 0.9861 - val_binary_accuracy: 0.8900\n","Epoch 45/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8294 - binary_accuracy: 0.9711 - val_loss: 0.9792 - val_binary_accuracy: 0.8867\n","Epoch 46/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8190 - binary_accuracy: 0.9744 - val_loss: 0.9682 - val_binary_accuracy: 0.8867\n","Epoch 47/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8088 - binary_accuracy: 0.9744 - val_loss: 0.9617 - val_binary_accuracy: 0.8867\n","Epoch 48/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7989 - binary_accuracy: 0.9733 - val_loss: 0.9550 - val_binary_accuracy: 0.8867\n","Epoch 49/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7890 - binary_accuracy: 0.9767 - val_loss: 0.9459 - val_binary_accuracy: 0.8867\n","Epoch 50/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7793 - binary_accuracy: 0.9756 - val_loss: 0.9393 - val_binary_accuracy: 0.8867\n","Epoch 1/50\n","36/36 [==============================] - 2s 8ms/step - loss: 1.6292 - binary_accuracy: 0.4922 - val_loss: 1.6221 - val_binary_accuracy: 0.5200\n","Epoch 2/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.6100 - binary_accuracy: 0.5656 - val_loss: 1.6078 - val_binary_accuracy: 0.5600\n","Epoch 3/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5895 - binary_accuracy: 0.6389 - val_loss: 1.5919 - val_binary_accuracy: 0.6033\n","Epoch 4/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5664 - binary_accuracy: 0.7333 - val_loss: 1.5740 - val_binary_accuracy: 0.6833\n","Epoch 5/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5411 - binary_accuracy: 0.8033 - val_loss: 1.5543 - val_binary_accuracy: 0.7133\n","Epoch 6/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5142 - binary_accuracy: 0.8444 - val_loss: 1.5329 - val_binary_accuracy: 0.7467\n","Epoch 7/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4859 - binary_accuracy: 0.8633 - val_loss: 1.5090 - val_binary_accuracy: 0.7633\n","Epoch 8/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4559 - binary_accuracy: 0.8833 - val_loss: 1.4843 - val_binary_accuracy: 0.8000\n","Epoch 9/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4261 - binary_accuracy: 0.8933 - val_loss: 1.4600 - val_binary_accuracy: 0.8133\n","Epoch 10/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3970 - binary_accuracy: 0.8967 - val_loss: 1.4370 - val_binary_accuracy: 0.8233\n","Epoch 11/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3685 - binary_accuracy: 0.9033 - val_loss: 1.4146 - val_binary_accuracy: 0.8167\n","Epoch 12/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3407 - binary_accuracy: 0.9111 - val_loss: 1.3932 - val_binary_accuracy: 0.8367\n","Epoch 13/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3141 - binary_accuracy: 0.9133 - val_loss: 1.3723 - val_binary_accuracy: 0.8467\n","Epoch 14/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2888 - binary_accuracy: 0.9111 - val_loss: 1.3523 - val_binary_accuracy: 0.8500\n","Epoch 15/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2640 - binary_accuracy: 0.9178 - val_loss: 1.3330 - val_binary_accuracy: 0.8533\n","Epoch 16/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2408 - binary_accuracy: 0.9233 - val_loss: 1.3147 - val_binary_accuracy: 0.8567\n","Epoch 17/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2180 - binary_accuracy: 0.9367 - val_loss: 1.2966 - val_binary_accuracy: 0.8600\n","Epoch 18/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1962 - binary_accuracy: 0.9333 - val_loss: 1.2795 - val_binary_accuracy: 0.8633\n","Epoch 19/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1748 - binary_accuracy: 0.9367 - val_loss: 1.2627 - val_binary_accuracy: 0.8700\n","Epoch 20/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1546 - binary_accuracy: 0.9400 - val_loss: 1.2466 - val_binary_accuracy: 0.8700\n","Epoch 21/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1346 - binary_accuracy: 0.9422 - val_loss: 1.2308 - val_binary_accuracy: 0.8800\n","Epoch 22/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1155 - binary_accuracy: 0.9456 - val_loss: 1.2154 - val_binary_accuracy: 0.8800\n","Epoch 23/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0970 - binary_accuracy: 0.9478 - val_loss: 1.2008 - val_binary_accuracy: 0.8700\n","Epoch 24/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0791 - binary_accuracy: 0.9489 - val_loss: 1.1868 - val_binary_accuracy: 0.8767\n","Epoch 25/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0616 - binary_accuracy: 0.9533 - val_loss: 1.1728 - val_binary_accuracy: 0.8800\n","Epoch 26/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0447 - binary_accuracy: 0.9578 - val_loss: 1.1598 - val_binary_accuracy: 0.8767\n","Epoch 27/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0285 - binary_accuracy: 0.9567 - val_loss: 1.1468 - val_binary_accuracy: 0.8833\n","Epoch 28/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0127 - binary_accuracy: 0.9600 - val_loss: 1.1355 - val_binary_accuracy: 0.8733\n","Epoch 29/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9975 - binary_accuracy: 0.9589 - val_loss: 1.1225 - val_binary_accuracy: 0.8767\n","Epoch 30/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9830 - binary_accuracy: 0.9611 - val_loss: 1.1108 - val_binary_accuracy: 0.8833\n","Epoch 31/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9685 - binary_accuracy: 0.9667 - val_loss: 1.0998 - val_binary_accuracy: 0.8800\n","Epoch 32/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9546 - binary_accuracy: 0.9667 - val_loss: 1.0890 - val_binary_accuracy: 0.8800\n","Epoch 33/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9410 - binary_accuracy: 0.9667 - val_loss: 1.0778 - val_binary_accuracy: 0.8867\n","Epoch 34/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9278 - binary_accuracy: 0.9667 - val_loss: 1.0678 - val_binary_accuracy: 0.8800\n","Epoch 35/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9150 - binary_accuracy: 0.9667 - val_loss: 1.0583 - val_binary_accuracy: 0.8800\n","Epoch 36/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9026 - binary_accuracy: 0.9722 - val_loss: 1.0476 - val_binary_accuracy: 0.8867\n","Epoch 37/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8901 - binary_accuracy: 0.9678 - val_loss: 1.0383 - val_binary_accuracy: 0.8833\n","Epoch 38/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8787 - binary_accuracy: 0.9689 - val_loss: 1.0295 - val_binary_accuracy: 0.8833\n","Epoch 39/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8670 - binary_accuracy: 0.9722 - val_loss: 1.0200 - val_binary_accuracy: 0.8833\n","Epoch 40/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8557 - binary_accuracy: 0.9722 - val_loss: 1.0112 - val_binary_accuracy: 0.8867\n","Epoch 41/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8449 - binary_accuracy: 0.9733 - val_loss: 1.0029 - val_binary_accuracy: 0.8800\n","Epoch 42/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8341 - binary_accuracy: 0.9744 - val_loss: 0.9944 - val_binary_accuracy: 0.8867\n","Epoch 43/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8236 - binary_accuracy: 0.9756 - val_loss: 0.9869 - val_binary_accuracy: 0.8800\n","Epoch 44/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8133 - binary_accuracy: 0.9744 - val_loss: 0.9786 - val_binary_accuracy: 0.8800\n","Epoch 45/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8032 - binary_accuracy: 0.9744 - val_loss: 0.9706 - val_binary_accuracy: 0.8800\n","Epoch 46/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7935 - binary_accuracy: 0.9800 - val_loss: 0.9632 - val_binary_accuracy: 0.8800\n","Epoch 47/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7836 - binary_accuracy: 0.9778 - val_loss: 0.9555 - val_binary_accuracy: 0.8800\n","Epoch 48/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7744 - binary_accuracy: 0.9800 - val_loss: 0.9482 - val_binary_accuracy: 0.8833\n","Epoch 49/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7651 - binary_accuracy: 0.9822 - val_loss: 0.9409 - val_binary_accuracy: 0.8800\n","Epoch 50/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7559 - binary_accuracy: 0.9811 - val_loss: 0.9342 - val_binary_accuracy: 0.8800\n","Epoch 1/50\n","36/36 [==============================] - 2s 8ms/step - loss: 1.6205 - binary_accuracy: 0.5600 - val_loss: 1.6149 - val_binary_accuracy: 0.5700\n","Epoch 2/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5958 - binary_accuracy: 0.6433 - val_loss: 1.5973 - val_binary_accuracy: 0.6367\n","Epoch 3/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5685 - binary_accuracy: 0.7256 - val_loss: 1.5790 - val_binary_accuracy: 0.6833\n","Epoch 4/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5389 - binary_accuracy: 0.7922 - val_loss: 1.5576 - val_binary_accuracy: 0.7100\n","Epoch 5/50\n","36/36 [==============================] - 0s 5ms/step - loss: 1.5081 - binary_accuracy: 0.8367 - val_loss: 1.5357 - val_binary_accuracy: 0.7433\n","Epoch 6/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4763 - binary_accuracy: 0.8656 - val_loss: 1.5126 - val_binary_accuracy: 0.7667\n","Epoch 7/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4456 - binary_accuracy: 0.8856 - val_loss: 1.4897 - val_binary_accuracy: 0.7600\n","Epoch 8/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4148 - binary_accuracy: 0.9022 - val_loss: 1.4690 - val_binary_accuracy: 0.7767\n","Epoch 9/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3853 - binary_accuracy: 0.9133 - val_loss: 1.4468 - val_binary_accuracy: 0.7733\n","Epoch 10/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3562 - binary_accuracy: 0.9233 - val_loss: 1.4252 - val_binary_accuracy: 0.7800\n","Epoch 11/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3288 - binary_accuracy: 0.9289 - val_loss: 1.4039 - val_binary_accuracy: 0.8033\n","Epoch 12/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3019 - binary_accuracy: 0.9378 - val_loss: 1.3852 - val_binary_accuracy: 0.8000\n","Epoch 13/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2759 - binary_accuracy: 0.9400 - val_loss: 1.3669 - val_binary_accuracy: 0.8067\n","Epoch 14/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2514 - binary_accuracy: 0.9456 - val_loss: 1.3489 - val_binary_accuracy: 0.8067\n","Epoch 15/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2274 - binary_accuracy: 0.9500 - val_loss: 1.3296 - val_binary_accuracy: 0.8167\n","Epoch 16/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2048 - binary_accuracy: 0.9500 - val_loss: 1.3128 - val_binary_accuracy: 0.8200\n","Epoch 17/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1829 - binary_accuracy: 0.9533 - val_loss: 1.2969 - val_binary_accuracy: 0.8200\n","Epoch 18/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1623 - binary_accuracy: 0.9567 - val_loss: 1.2814 - val_binary_accuracy: 0.8267\n","Epoch 19/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1422 - binary_accuracy: 0.9589 - val_loss: 1.2665 - val_binary_accuracy: 0.8267\n","Epoch 20/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1230 - binary_accuracy: 0.9600 - val_loss: 1.2523 - val_binary_accuracy: 0.8200\n","Epoch 21/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1048 - binary_accuracy: 0.9656 - val_loss: 1.2409 - val_binary_accuracy: 0.8233\n","Epoch 22/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0872 - binary_accuracy: 0.9633 - val_loss: 1.2281 - val_binary_accuracy: 0.8233\n","Epoch 23/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0698 - binary_accuracy: 0.9644 - val_loss: 1.2131 - val_binary_accuracy: 0.8233\n","Epoch 24/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0537 - binary_accuracy: 0.9656 - val_loss: 1.2019 - val_binary_accuracy: 0.8267\n","Epoch 25/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0377 - binary_accuracy: 0.9644 - val_loss: 1.1916 - val_binary_accuracy: 0.8200\n","Epoch 26/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0225 - binary_accuracy: 0.9700 - val_loss: 1.1796 - val_binary_accuracy: 0.8267\n","Epoch 27/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0077 - binary_accuracy: 0.9689 - val_loss: 1.1697 - val_binary_accuracy: 0.8200\n","Epoch 28/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9930 - binary_accuracy: 0.9711 - val_loss: 1.1580 - val_binary_accuracy: 0.8300\n","Epoch 29/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9793 - binary_accuracy: 0.9689 - val_loss: 1.1473 - val_binary_accuracy: 0.8333\n","Epoch 30/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9656 - binary_accuracy: 0.9711 - val_loss: 1.1374 - val_binary_accuracy: 0.8367\n","Epoch 31/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9527 - binary_accuracy: 0.9711 - val_loss: 1.1292 - val_binary_accuracy: 0.8400\n","Epoch 32/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9397 - binary_accuracy: 0.9733 - val_loss: 1.1214 - val_binary_accuracy: 0.8367\n","Epoch 33/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9273 - binary_accuracy: 0.9756 - val_loss: 1.1103 - val_binary_accuracy: 0.8400\n","Epoch 34/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9150 - binary_accuracy: 0.9756 - val_loss: 1.1045 - val_binary_accuracy: 0.8333\n","Epoch 35/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9034 - binary_accuracy: 0.9744 - val_loss: 1.0943 - val_binary_accuracy: 0.8433\n","Epoch 36/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8918 - binary_accuracy: 0.9767 - val_loss: 1.0862 - val_binary_accuracy: 0.8433\n","Epoch 37/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8805 - binary_accuracy: 0.9778 - val_loss: 1.0789 - val_binary_accuracy: 0.8433\n","Epoch 38/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8695 - binary_accuracy: 0.9778 - val_loss: 1.0689 - val_binary_accuracy: 0.8433\n","Epoch 39/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8588 - binary_accuracy: 0.9789 - val_loss: 1.0621 - val_binary_accuracy: 0.8433\n","Epoch 40/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8483 - binary_accuracy: 0.9811 - val_loss: 1.0525 - val_binary_accuracy: 0.8467\n","Epoch 41/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8379 - binary_accuracy: 0.9800 - val_loss: 1.0485 - val_binary_accuracy: 0.8467\n","Epoch 42/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8279 - binary_accuracy: 0.9811 - val_loss: 1.0397 - val_binary_accuracy: 0.8433\n","Epoch 43/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8179 - binary_accuracy: 0.9822 - val_loss: 1.0329 - val_binary_accuracy: 0.8433\n","Epoch 44/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8082 - binary_accuracy: 0.9811 - val_loss: 1.0244 - val_binary_accuracy: 0.8433\n","Epoch 45/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7987 - binary_accuracy: 0.9844 - val_loss: 1.0193 - val_binary_accuracy: 0.8433\n","Epoch 46/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7892 - binary_accuracy: 0.9833 - val_loss: 1.0113 - val_binary_accuracy: 0.8433\n","Epoch 47/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7799 - binary_accuracy: 0.9844 - val_loss: 1.0034 - val_binary_accuracy: 0.8533\n","Epoch 48/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7709 - binary_accuracy: 0.9844 - val_loss: 0.9993 - val_binary_accuracy: 0.8467\n","Epoch 49/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7620 - binary_accuracy: 0.9844 - val_loss: 0.9924 - val_binary_accuracy: 0.8433\n","Epoch 50/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.7531 - binary_accuracy: 0.9856 - val_loss: 0.9862 - val_binary_accuracy: 0.8467\n","Epoch 1/50\n","36/36 [==============================] - 2s 9ms/step - loss: 1.6297 - binary_accuracy: 0.5133 - val_loss: 1.6187 - val_binary_accuracy: 0.5900\n","Epoch 2/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.6152 - binary_accuracy: 0.5656 - val_loss: 1.6066 - val_binary_accuracy: 0.5967\n","Epoch 3/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.6006 - binary_accuracy: 0.5878 - val_loss: 1.5932 - val_binary_accuracy: 0.6400\n","Epoch 4/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5841 - binary_accuracy: 0.6111 - val_loss: 1.5772 - val_binary_accuracy: 0.6633\n","Epoch 5/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5658 - binary_accuracy: 0.6344 - val_loss: 1.5608 - val_binary_accuracy: 0.6700\n","Epoch 6/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5469 - binary_accuracy: 0.6700 - val_loss: 1.5441 - val_binary_accuracy: 0.6867\n","Epoch 7/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5275 - binary_accuracy: 0.7022 - val_loss: 1.5272 - val_binary_accuracy: 0.6933\n","Epoch 8/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.5078 - binary_accuracy: 0.7367 - val_loss: 1.5113 - val_binary_accuracy: 0.7167\n","Epoch 9/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4878 - binary_accuracy: 0.7667 - val_loss: 1.4960 - val_binary_accuracy: 0.7233\n","Epoch 10/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4683 - binary_accuracy: 0.7989 - val_loss: 1.4810 - val_binary_accuracy: 0.7500\n","Epoch 11/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4482 - binary_accuracy: 0.8189 - val_loss: 1.4665 - val_binary_accuracy: 0.7333\n","Epoch 12/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4280 - binary_accuracy: 0.8400 - val_loss: 1.4519 - val_binary_accuracy: 0.7600\n","Epoch 13/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.4081 - binary_accuracy: 0.8711 - val_loss: 1.4354 - val_binary_accuracy: 0.7600\n","Epoch 14/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3878 - binary_accuracy: 0.8844 - val_loss: 1.4202 - val_binary_accuracy: 0.7767\n","Epoch 15/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3677 - binary_accuracy: 0.8867 - val_loss: 1.4066 - val_binary_accuracy: 0.8000\n","Epoch 16/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3478 - binary_accuracy: 0.9122 - val_loss: 1.3915 - val_binary_accuracy: 0.8067\n","Epoch 17/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3276 - binary_accuracy: 0.9233 - val_loss: 1.3755 - val_binary_accuracy: 0.8067\n","Epoch 18/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.3073 - binary_accuracy: 0.9300 - val_loss: 1.3599 - val_binary_accuracy: 0.8067\n","Epoch 19/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2874 - binary_accuracy: 0.9333 - val_loss: 1.3449 - val_binary_accuracy: 0.8033\n","Epoch 20/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2675 - binary_accuracy: 0.9356 - val_loss: 1.3303 - val_binary_accuracy: 0.8033\n","Epoch 21/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2475 - binary_accuracy: 0.9356 - val_loss: 1.3175 - val_binary_accuracy: 0.8133\n","Epoch 22/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2276 - binary_accuracy: 0.9456 - val_loss: 1.3021 - val_binary_accuracy: 0.8133\n","Epoch 23/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.2082 - binary_accuracy: 0.9422 - val_loss: 1.2885 - val_binary_accuracy: 0.8133\n","Epoch 24/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1888 - binary_accuracy: 0.9444 - val_loss: 1.2749 - val_binary_accuracy: 0.8133\n","Epoch 25/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1697 - binary_accuracy: 0.9456 - val_loss: 1.2613 - val_binary_accuracy: 0.8200\n","Epoch 26/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1512 - binary_accuracy: 0.9500 - val_loss: 1.2497 - val_binary_accuracy: 0.8133\n","Epoch 27/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1330 - binary_accuracy: 0.9511 - val_loss: 1.2355 - val_binary_accuracy: 0.8200\n","Epoch 28/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.1151 - binary_accuracy: 0.9533 - val_loss: 1.2240 - val_binary_accuracy: 0.8133\n","Epoch 29/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0975 - binary_accuracy: 0.9556 - val_loss: 1.2130 - val_binary_accuracy: 0.8167\n","Epoch 30/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0804 - binary_accuracy: 0.9578 - val_loss: 1.1987 - val_binary_accuracy: 0.8233\n","Epoch 31/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0638 - binary_accuracy: 0.9556 - val_loss: 1.1893 - val_binary_accuracy: 0.8200\n","Epoch 32/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0473 - binary_accuracy: 0.9611 - val_loss: 1.1747 - val_binary_accuracy: 0.8267\n","Epoch 33/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0316 - binary_accuracy: 0.9611 - val_loss: 1.1649 - val_binary_accuracy: 0.8300\n","Epoch 34/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0158 - binary_accuracy: 0.9622 - val_loss: 1.1532 - val_binary_accuracy: 0.8300\n","Epoch 35/50\n","36/36 [==============================] - 0s 4ms/step - loss: 1.0007 - binary_accuracy: 0.9656 - val_loss: 1.1448 - val_binary_accuracy: 0.8200\n","Epoch 36/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9858 - binary_accuracy: 0.9667 - val_loss: 1.1325 - val_binary_accuracy: 0.8300\n","Epoch 37/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9713 - binary_accuracy: 0.9678 - val_loss: 1.1224 - val_binary_accuracy: 0.8267\n","Epoch 38/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9573 - binary_accuracy: 0.9689 - val_loss: 1.1121 - val_binary_accuracy: 0.8267\n","Epoch 39/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9435 - binary_accuracy: 0.9678 - val_loss: 1.1045 - val_binary_accuracy: 0.8233\n","Epoch 40/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9301 - binary_accuracy: 0.9678 - val_loss: 1.0951 - val_binary_accuracy: 0.8233\n","Epoch 41/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9169 - binary_accuracy: 0.9678 - val_loss: 1.0860 - val_binary_accuracy: 0.8200\n","Epoch 42/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9043 - binary_accuracy: 0.9722 - val_loss: 1.0766 - val_binary_accuracy: 0.8233\n","Epoch 43/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8919 - binary_accuracy: 0.9744 - val_loss: 1.0663 - val_binary_accuracy: 0.8233\n","Epoch 44/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8798 - binary_accuracy: 0.9733 - val_loss: 1.0587 - val_binary_accuracy: 0.8267\n","Epoch 45/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8679 - binary_accuracy: 0.9722 - val_loss: 1.0503 - val_binary_accuracy: 0.8267\n","Epoch 46/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8563 - binary_accuracy: 0.9744 - val_loss: 1.0437 - val_binary_accuracy: 0.8300\n","Epoch 47/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8451 - binary_accuracy: 0.9744 - val_loss: 1.0363 - val_binary_accuracy: 0.8300\n","Epoch 48/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8343 - binary_accuracy: 0.9767 - val_loss: 1.0276 - val_binary_accuracy: 0.8300\n","Epoch 49/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8231 - binary_accuracy: 0.9756 - val_loss: 1.0177 - val_binary_accuracy: 0.8300\n","Epoch 50/50\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8127 - binary_accuracy: 0.9767 - val_loss: 1.0144 - val_binary_accuracy: 0.8367\n"]}]},{"cell_type":"markdown","source":["*Q: What does 'x' represent in these iterators?*\n","\n","*Q: Can anyone explain in simple words what this code is doing?*"],"metadata":{"id":"ERQl3xUL2K1Y"}},{"cell_type":"code","source":["average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n","average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n","average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n","average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]"],"metadata":{"id":"OIdUNbd02LoL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we plot the cross-validated performance:"],"metadata":{"id":"Ba1kaeAd2gax"}},{"cell_type":"code","source":["# Plot validation performance. \n","plt.plot(average_loss_history,c='r')\n","plt.plot(average_acc_history,c=\"r\",linestyle=\"dashed\")\n","plt.plot(average_val_loss_history,c='b')\n","plt.plot(average_val_acc_history,c='b',linestyle=\"dashed\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend(['Training Loss','Training Accuracy','Validation Loss','Validation Accuracy'])\n","plt.show()"],"metadata":{"id":"rRmEzDzHVeh9"},"execution_count":null,"outputs":[]}]}